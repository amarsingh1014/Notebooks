{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpfxOyJ0s2xrYHWDGvrJww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yD1kBdv8r-WC","executionInfo":{"status":"ok","timestamp":1743080465345,"user_tz":-330,"elapsed":49,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}}},"outputs":[],"source":["sentence = \"\"\"Positional Encoding is a technique used in Natural Language Processing (NLP) to inject information about the order of tokens in a sequence.\n","It is essential in Transformer models, like BERT and GPT, since they process tokens in parallel and lack inherent sequential order information (unlike RNNs or LSTMs).\n","\"\"\""]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6zBAn0Cs5Xs","executionInfo":{"status":"ok","timestamp":1743080629035,"user_tz":-330,"elapsed":118,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"a678f78c-ce7b-43d5-8491-1595e1b868a1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from nltk.corpus import stopwords"],"metadata":{"id":"zZeywxqBsTbR","executionInfo":{"status":"ok","timestamp":1743080634045,"user_tz":-330,"elapsed":17,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words('english'))\n","\n","list(stop_words)[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1j35ZuLsY8b","executionInfo":{"status":"ok","timestamp":1743080746020,"user_tz":-330,"elapsed":64,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"91dfebd7-699b-4c7f-8517-81d9169f96a6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['at',\n"," \"hadn't\",\n"," \"that'll\",\n"," 'there',\n"," 'he',\n"," 'into',\n"," 'you',\n"," 'theirs',\n"," 'were',\n"," 'again']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["sentence = sentence.lower().split()"],"metadata":{"id":"8FhAKnqktdIw","executionInfo":{"status":"ok","timestamp":1743080852670,"user_tz":-330,"elapsed":3,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["sentence_no_stopwords = [word for word in sentence if word not in stop_words]"],"metadata":{"id":"BmDE2zJwtzRr","executionInfo":{"status":"ok","timestamp":1743080940456,"user_tz":-330,"elapsed":9,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["sentence_no_stopwords"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cu1XA_0uG5w","executionInfo":{"status":"ok","timestamp":1743080951909,"user_tz":-330,"elapsed":17,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"11610f45-348c-4587-8ddf-2279b83b35ab"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['positional',\n"," 'encoding',\n"," 'technique',\n"," 'used',\n"," 'natural',\n"," 'language',\n"," 'processing',\n"," '(nlp)',\n"," 'inject',\n"," 'information',\n"," 'order',\n"," 'tokens',\n"," 'sequence.',\n"," 'essential',\n"," 'transformer',\n"," 'models,',\n"," 'like',\n"," 'bert',\n"," 'gpt,',\n"," 'since',\n"," 'process',\n"," 'tokens',\n"," 'parallel',\n"," 'lack',\n"," 'inherent',\n"," 'sequential',\n"," 'order',\n"," 'information',\n"," '(unlike',\n"," 'rnns',\n"," 'lstms).']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["print(' '.join(sentence_no_stopwords))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bmtx-Y7AuOXf","executionInfo":{"status":"ok","timestamp":1743081002505,"user_tz":-330,"elapsed":25,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"2c811554-352c-4c52-b52d-bf214cf008e0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["positional encoding technique used natural language processing (nlp) inject information order tokens sequence. essential transformer models, like bert gpt, since process tokens parallel lack inherent sequential order information (unlike rnns lstms).\n"]}]}]}