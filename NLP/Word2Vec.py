# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ZsZwN7JZdBmR2DuW82BS97IOhQZ3VH3
"""

!pip install torch tqdm

import os, io, zipfile, requests, tempfile
import numpy as np
import torch
from torch.utils.data import DataLoader, IterableDataset
from torch.cuda.amp import autocast, GradScaler
from collections import Counter
import random, time

# Download text8
url = "http://mattmahoney.net/dc/text8.zip"
print("Downloading text8...")
resp = requests.get(url)
z = zipfile.ZipFile(io.BytesIO(resp.content))

stream = z.open("text8")  # binary stream

OUT_SENT_FILE = "text8_chunks.txt"
CHUNK_SIZE = 1000

print("Streaming text8 → chunked sentence file...")

with open(OUT_SENT_FILE, "w", encoding="utf-8") as fout:
    buf = []
    for chunk in iter(lambda: stream.read(4096), b""):
        text = chunk.decode(errors="ignore")
        for w in text.split():
            buf.append(w)
            if len(buf) >= CHUNK_SIZE:
                fout.write(" ".join(buf) + "\n")
                buf = []
    if buf:
        fout.write(" ".join(buf) + "\n")

print("Done. Written:", OUT_SENT_FILE)

def build_vocab(path, min_count=5):
    counter = Counter()
    with open(path, "r") as f:
        for line in f:
            counter.update(line.split())

    items = [(w,c) for w,c in counter.items() if c >= min_count]
    items.sort(key=lambda x:(-x[1], x[0]))

    word2idx = {w:i for i,(w,_) in enumerate(items)}
    idx2word = {i:w for w,i in word2idx.items()}

    print("Vocab size:", len(word2idx))
    return word2idx, idx2word, counter

word2idx, idx2word, counter = build_vocab("text8_chunks.txt")

def build_neg_table_memmap(word2idx, counter, power=0.75, table_size=2_000_000):
    vocab_size = len(word2idx)
    freqs = np.zeros(vocab_size, dtype=np.float64)

    for w, idx in word2idx.items():
        freqs[idx] = counter[w]

    freqs = freqs ** power
    freqs = freqs / freqs.sum()

    fname = os.path.join(tempfile.gettempdir(), "neg_table.dat")
    if os.path.exists(fname):
        os.remove(fname)

    table = np.memmap(fname, dtype=np.int32, mode='w+', shape=(table_size,))
    cumulative = np.cumsum(freqs)

    j = 0
    for i in range(table_size):
        x = (i+0.5)/table_size
        while x > cumulative[j]:
            j += 1
        table[i] = j

    table.flush()
    return table

neg_table = build_neg_table_memmap(word2idx, counter, table_size=2_000_000)
print("Neg table:", neg_table.shape)

class StreamingSkipGramDataset(IterableDataset):
    def __init__(self, file_path, word2idx, window=2, K=5, neg_table=None, seed=42):
        self.file_path = file_path
        self.word2idx = word2idx
        self.window = window
        self.K = K
        self.neg_table = neg_table
        self.seed = seed

    def __iter__(self):
        rng = np.random.RandomState(self.seed)
        with open(self.file_path, "r") as f:
            for line in f:
                ids = [self.word2idx[w] for w in line.split() if w in self.word2idx]
                L = len(ids)
                for i, center in enumerate(ids):
                    left = max(0, i - self.window)
                    right = min(L, i + self.window + 1)
                    for j in range(left, right):
                        if j == i:
                            continue
                        pos = ids[j]
                        # sample K negatives
                        neg_idx = rng.randint(0, len(self.neg_table), size=self.K)
                        negs = self.neg_table[neg_idx].astype(np.int64)
                        yield center, pos, negs

def collate_sg(batch):
    c = torch.tensor([b[0] for b in batch], dtype=torch.long)
    p = torch.tensor([b[1] for b in batch], dtype=torch.long)
    n = torch.tensor([b[2] for b in batch], dtype=torch.long)
    return c, p, n

BATCH_SIZE = 16384  # safe starting point
K = 5

dataset = StreamingSkipGramDataset("text8_chunks.txt", word2idx, window=2, K=K, neg_table=neg_table)

loader = DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    collate_fn=collate_sg,
    num_workers=2,           # increase if needed
    pin_memory=True,
    drop_last=True,
    persistent_workers=True
)

# Test one batch
for c, p, n in loader:
    print(c.shape, p.shape, n.shape)
    break

import torch.nn as nn
import torch.nn.functional as F

class SkipGramNS(nn.Module):
    def __init__(self, vocab_size, embed_dim, K):
        super().__init__()
        self.K = K
        self.in_embeddings = nn.Embedding(vocab_size, embed_dim, sparse=True)
        self.out_embeddings = nn.Embedding(vocab_size, embed_dim, sparse=True)

        init_range = 0.5 / embed_dim
        self.in_embeddings.weight.data.uniform_(-init_range, init_range)
        self.out_embeddings.weight.data.uniform_(-0, 0)

    def forward(self, center, pos, neg):
        B = center.size(0)

        v_c = self.in_embeddings(center)    # (B, D)
        v_p = self.out_embeddings(pos)      # (B, D)
        v_n = self.out_embeddings(neg)      # (B, K, D)

        pos_score = torch.sum(v_c * v_p, dim=1)      # (B,)
        pos_loss = F.logsigmoid(pos_score).sum()

        neg_score = torch.bmm(v_n, v_c.unsqueeze(2)).squeeze()  # (B, K)
        neg_loss = F.logsigmoid(-neg_score).sum()

        loss = -(pos_loss + neg_loss) / B
        return loss

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using:", DEVICE)

embed_dim = 100
model = SkipGramNS(len(word2idx), embed_dim, K).to(DEVICE)

optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.002)
scaler = GradScaler()

EPOCHS = 3
log_interval = 2000

for epoch in range(1, EPOCHS+1):
    total_loss = 0
    count = 0
    t0 = time.time()

    for batch_idx, (c, p, n) in enumerate(loader):

        c = c.to(DEVICE, non_blocking=True)
        p = p.to(DEVICE, non_blocking=True)
        n = n.to(DEVICE, non_blocking=True)

        optimizer.zero_grad()

        with autocast():
            loss = model(c, p, n)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        total_loss += loss.item()
        count += 1

        if batch_idx % log_interval == 0 and batch_idx > 0:
            print(f"Epoch {epoch} | Batch {batch_idx} | Loss {total_loss/count:.4f}")
            total_loss = 0
            count = 0

    print(f"✓ Finished Epoch {epoch} in {time.time()-t0:.1f}s")

torch.save({
    "in_embeddings": model.in_embeddings.weight.data.cpu(),
    "word2idx": word2idx,
    "idx2word": idx2word
}, "word2vec_text8.pth")

print("Saved word2vec_text8.pth")

data = torch.load("word2vec_text8.pth")
vecs = data["in_embeddings"]
word2idx = data["word2idx"]
idx2word = data["idx2word"]

def nearest(word, topn=10):
    if word not in word2idx:
        print("Word not in vocab"); return
    idx = word2idx[word]
    v = vecs[idx]
    v = v / v.norm()
    all_vecs = vecs / vecs.norm(dim=1, keepdim=True)
    sims = torch.matmul(all_vecs, v)
    top = torch.topk(sims, topn+1).indices.tolist()
    print("Nearest to:", word)
    for i in top[1:]:
        print(idx2word[i])

nearest("president")