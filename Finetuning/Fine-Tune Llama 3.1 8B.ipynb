{"cells":[{"cell_type":"markdown","metadata":{"id":"aTaDCGTe78bK"},"source":["# DataCamp - Fine-tune Llama 3.1 8B\n","> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n","\n","‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne).\n","\n","Add `HF_TOKEN` in the Secrets tab to store your [Hugging Face access token](https://huggingface.co/settings/tokens) in Colab.\n","\n","![](https://i.imgur.com/VyPwxqa.png)\n","![](https://i.imgur.com/LXdQpUh.png)\n","![](https://i.imgur.com/urRLLyC.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoPKQjga6obN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757559446693,"user_tz":-330,"elapsed":78221,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"5988e90c-152e-4c78-a30b-464924e9d77b"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-695731857.py:10: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth.chat_templates import get_chat_template\n"]},{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n","from torch import __version__; from packaging.version import Version as V\n","xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n","!pip install -qqq --no-deps {xformers} trl peft accelerate bitsandbytes triton --progress-bar off\n","\n","import torch\n","from trl import SFTTrainer\n","from datasets import load_dataset\n","from transformers import TrainingArguments, TextStreamer\n","from unsloth.chat_templates import get_chat_template\n","from unsloth import FastLanguageModel, is_bfloat16_supported"]},{"cell_type":"markdown","metadata":{"id":"matKaF-f-GiU"},"source":["## 1. Load model for PEFT\n","\n","![](https://i.imgur.com/2CgewGd.png)\n","![](https://i.imgur.com/Y8qsNvf.png)\n","\n","We load the model using parameter-efficient techniques (PEFT) to reduce VRAM usage and speed up training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGX9wG7Lhc-z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757559482090,"user_tz":-330,"elapsed":35389,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"c731e391-8d45-40f2-a3e0-c595bd8eea36"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.9.4: Fast Llama patching. Transformers: 4.56.1.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]}],"source":["max_seq_length = 2048\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n","    max_seq_length=max_seq_length,\n","    load_in_4bit=True,\n","    dtype=None,\n",")"]},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model=model,\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0,\n","    target_modules=[\"q_proj\",\"k_proj\", \"v_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n","    use_rslora=True,\n","    use_gradient_checkpointing=\"unsloth\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bp50OZFkJuqH","executionInfo":{"status":"ok","timestamp":1757559495482,"user_tz":-330,"elapsed":13401,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"71626eb7-81ad-45da-9930-a57fdc6307d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n","are not enabled or a bias term (like in Qwen) is used.\n","Unsloth 2025.9.4 patched 32 layers with 32 QKV layers, 32 O layers and 0 MLP layers.\n"]}]},{"cell_type":"markdown","source":["## 2. Prepare data and tokenizer\n","\n","![](https://i.imgur.com/cIGv8Cb.png)\n","![](https://i.imgur.com/FFxWTbK.png)\n","![](https://i.imgur.com/a3navcZ.png)\n","\n","We prepare our instruction dataset with the right chat template and tokenizer."],"metadata":{"id":"hjDpwfjJ3RAL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqGnvaT8is-R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1757559498751,"user_tz":-330,"elapsed":3265,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"ee8b180c-e194-4ada-9493-9cc4e255128b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth: Will map <|im_end|> to EOS = <|end_of_text|>.\n"]}],"source":["tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template=\"chatml\",\n","    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",")\n"]},{"cell_type":"code","source":["dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:200]\")"],"metadata":{"id":"RdSOlm2YQ3i-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_template(examples):\n","  messages = examples[\"conversations\"]\n","  text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n","  return {\"text\": text}\n","\n","dataset = dataset.map(apply_template, batched=True)"],"metadata":{"id":"TqUu9nKARhew"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Training\n","\n","![](https://i.imgur.com/D8sDuhK.png)\n","![](https://i.imgur.com/YeGVUup.png)\n","\n","We specify the hyperparameters and train our model using Unsloth."],"metadata":{"id":"zdfjufQd3XMi"}},{"cell_type":"code","source":["dataset[\"text\"][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"xaDuNgsTkZ9n","executionInfo":{"status":"ok","timestamp":1757559501220,"user_tz":-330,"elapsed":54,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"726c8ebd-bf0c-4bc2-ccfb-cb16a52447bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|im_start|>user\\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.<|im_end|>\\n<|im_start|>assistant\\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.<|im_end|>\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcPAQihcjcfl","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["cb080851fb8f45139e4f32492e8b55c0","48e2f241850a4467abf47cce53000453","d4afecd380ec4618813f14003303d8bc","dd09eaaf3a684771b0b321cc7a5aa45d","539d780c8eba443c8a52c3d7d962150b","830b6f85599246cc8f7aa116ab7e47bc","826043b8fdf74751af59d3212685a9f5","a88494fbe6054d868edf2cdd88314f8a","6326a4947f734a19a320db281a587de7","d5e039a8746b425a8d6458a6b27826e2","2714f420fe8942a9a82678816ab0c6db"]},"executionInfo":{"status":"ok","timestamp":1757559734517,"user_tz":-330,"elapsed":10277,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"e86a613d-256a-4d11-ade5-b69cc3d16d4d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/200 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb080851fb8f45139e4f32492e8b55c0"}},"metadata":{}}],"source":["from trl import SFTConfig, SFTTrainer\n","\n","# set small seq len for memory safety\n","max_seq_length = 1024\n","\n","sft_config = SFTConfig(\n","    max_seq_length=max_seq_length,\n","    packing=False,  # debug with packing off\n","    dataset_text_field=\"text\",\n","    # optimizer/other training args:\n","    learning_rate=3e-4,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=16,  # effective batch 16\n","    num_train_epochs=1,\n","    fp16=True,\n","    optim=\"adamw_8bit\",    # or 'paged_adamw_32bit' depending on bitsandbytes\n","    output_dir=\"output\",\n","    seed=0,\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    processing_class=tokenizer,   # as earlier advice\n","    args=sft_config\n",")\n","\n","# memory helpers\n","model.gradient_checkpointing_enable()\n","# optionally: torch.cuda.empty_cache()\n"]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":628},"id":"eyO67uiXm0Il","executionInfo":{"status":"ok","timestamp":1757560136396,"user_tz":-330,"elapsed":397770,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"f47c88f1-b333-4fde-a126-f52e37284e62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 200 | Num Epochs = 1 | Total steps = 13\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n","\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n"," \"-____-\"     Trainable parameters = 32,505,856 of 8,062,767,104 (0.40% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 06:03, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.299100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.225700</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.013900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.964300</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.879900</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.836000</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.773000</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.757000</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.733400</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.763500</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.846500</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.824600</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.696800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=13, training_loss=0.8933482995400062, metrics={'train_runtime': 395.168, 'train_samples_per_second': 0.506, 'train_steps_per_second': 0.033, 'total_flos': 5180928247726080.0, 'train_loss': 0.8933482995400062, 'epoch': 1.0})"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["## 4. Inference\n","\n","We test the trained model with a toy example to check that there's no obvious error."],"metadata":{"id":"CI_U9FHZ3ZLO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5JXdjsLqkvZY"},"outputs":[],"source":["messages = [\n","    {\"from\": \"human\", \"value\": \"Is 9.11 greater than 9.9?\"}\n","]\n","inputs = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=True,\n","    add_generation_prompt=True,\n","    return_tensors=\"pt\",\n",").to(\"cuda\")"]},{"cell_type":"code","source":["text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(inputs=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"],"metadata":{"id":"ohbgzBNqwbNZ","executionInfo":{"status":"ok","timestamp":1757560499547,"user_tz":-330,"elapsed":3435,"user":{"displayName":"Amar Pratap Singh","userId":"06114485624711689023"}},"outputId":"ec40e423-34d1-4187-c139-9355df09cf6b","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<|im_start|>user\n","Is 9.11 greater than 9.9?<|im_end|>\n","<|im_start|>assistant\n","Yes, 9.11 is greater than 9.9. The number 9.11 is larger than 9.9 because it has a larger decimal part.<|im_end|>\n"]}]},{"cell_type":"markdown","source":["## 5. Save trained model\n","\n","We save and export the trained model in safetensors and GGUF formats."],"metadata":{"id":"HunPZjPp3aWe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORa-rPvGmT9p"},"outputs":[],"source":["model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzcUOyksmgWH"},"outputs":[],"source":["model.push_to_hub_merged(\"mlabonne/LogicLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")"]},{"cell_type":"code","source":["model.push_to_hub_gguf(\"mlabonne/LogicLlama-3.1-8B-gguf\", tokenizer, \"q8_0\")"],"metadata":{"id":"AFXsCghZynGx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Next steps\n","\n","![](https://i.imgur.com/dMLEDKH.png)\n","![](https://i.imgur.com/jaOowAJ.png)\n","![](https://i.imgur.com/DlTKPHj.png)\n","![](https://i.imgur.com/EMBelvN.png)\n","![](https://i.imgur.com/QyUp4tA.png)"],"metadata":{"id":"NV3uadfUWdSL"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1NoVXxvXscJHzAHytSWTqPZ49YY_DecSd","timestamp":1757573116675},{"file_id":"1P2pNAsJcXyN6tNHz92PZ5o8zkrEqiWzY","timestamp":1724428549411},{"file_id":"164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z","timestamp":1724428061202},{"file_id":"16GkVYWq4CTmUzoL6-LR9hQloLpbaxCj-","timestamp":1722072326388},{"file_id":"1_e9wNG2hv4PjBfzVtg8hYmU8gZGgPL2u","timestamp":1721815509495}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cb080851fb8f45139e4f32492e8b55c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48e2f241850a4467abf47cce53000453","IPY_MODEL_d4afecd380ec4618813f14003303d8bc","IPY_MODEL_dd09eaaf3a684771b0b321cc7a5aa45d"],"layout":"IPY_MODEL_539d780c8eba443c8a52c3d7d962150b"}},"48e2f241850a4467abf47cce53000453":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_830b6f85599246cc8f7aa116ab7e47bc","placeholder":"‚Äã","style":"IPY_MODEL_826043b8fdf74751af59d3212685a9f5","value":"Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=6):‚Äá100%"}},"d4afecd380ec4618813f14003303d8bc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a88494fbe6054d868edf2cdd88314f8a","max":200,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6326a4947f734a19a320db281a587de7","value":200}},"dd09eaaf3a684771b0b321cc7a5aa45d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5e039a8746b425a8d6458a6b27826e2","placeholder":"‚Äã","style":"IPY_MODEL_2714f420fe8942a9a82678816ab0c6db","value":"‚Äá200/200‚Äá[00:09&lt;00:00,‚Äá41.04‚Äáexamples/s]"}},"539d780c8eba443c8a52c3d7d962150b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830b6f85599246cc8f7aa116ab7e47bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"826043b8fdf74751af59d3212685a9f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a88494fbe6054d868edf2cdd88314f8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6326a4947f734a19a320db281a587de7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5e039a8746b425a8d6458a6b27826e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2714f420fe8942a9a82678816ab0c6db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}